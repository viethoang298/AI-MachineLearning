{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwnnBfWCYonoRIlrLe5STZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viethoang298/AI-MachineLearning/blob/main/MachineLearning_PolynomialandLogistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polymonial Regression"
      ],
      "metadata": {
        "id": "4gBqiuYa0QL-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wm8ea000Y74a"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    Template for polynomial regression\n",
        "    AUTHOR Eric Eaton, Xiaoxiang Hu\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "#  Class PolynomialRegression\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "class PolynomialRegression:\n",
        "\n",
        "    def __init__(self, degree = 1, regLambda = 1E-8):\n",
        "        '''\n",
        "        Constructor\n",
        "        '''\n",
        "        #TODO\n",
        "        self.degree = degree\n",
        "        self.regLambda = regLambda\n",
        "\n",
        "\n",
        "    def polyfeatures(self, X, degree):\n",
        "        '''\n",
        "        Expands the given X into an n * d array of polynomial features of\n",
        "            degree d.\n",
        "\n",
        "        Returns:\n",
        "            A n-by-d numpy array, with each row comprising of\n",
        "            X, X * X, X ** 3, ... up to the dth power of X.\n",
        "            Note that the returned matrix will not inlude the zero-th power.\n",
        "\n",
        "        Arguments:\n",
        "            X is an n-by-1 column numpy array\n",
        "            degree is a positive integer\n",
        "        '''\n",
        "        #TODO\n",
        "\n",
        "        poly_X = np.zeros((X.shape[0], degree))\n",
        "        for i in range(degree):\n",
        "              poly_X[:, i] = X[:, 0] ** (i + 1)\n",
        "\n",
        "        # Return the polynomial features array\n",
        "        return poly_X\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "            Trains the model\n",
        "            Arguments:\n",
        "                X is a n-by-1 array\n",
        "                y is an n-by-1 array\n",
        "            Returns:\n",
        "                No return value\n",
        "            Note:\n",
        "                You need to apply polynomial expansion and scaling\n",
        "                at first\n",
        "        '''\n",
        "        #TODO\n",
        "        # Apply polynomial expansion to X\n",
        "        poly_X = self.polyfeatures(X, self.degree)\n",
        "\n",
        "        # Apply feature normalization to X\n",
        "        poly_X, self.mu, self.std = self.feature_normalize(poly_X)\n",
        "\n",
        "        # Add a column of ones to X to account for the intercept term\n",
        "        poly_X = np.c_[np.ones((X.shape[0], 1)), poly_X]\n",
        "\n",
        "        # Initialize theta to zero\n",
        "        self.theta = np.zeros((poly_X.shape[1], 1))\n",
        "\n",
        "        # Run gradient descent to optimize theta\n",
        "        self.theta, self.J_history = self.gradient_descent(poly_X, y, self.theta, self.alpha, self.n_iter)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Use the trained model to predict values for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-1 numpy array\n",
        "        Returns:\n",
        "            an n-by-1 numpy array of the predictions\n",
        "        '''\n",
        "        # TODO\n",
        "        n = len(X)\n",
        "\n",
        "        # add 1s column\n",
        "        Xex = np.c_[np.ones([n, 1]), X];\n",
        "\n",
        "        # predict\n",
        "        return Xex.dot(self.theta);\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "#  End of Class PolynomialRegression\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "\n",
        "def learningCurve(Xtrain, Ytrain, Xtest, Ytest, regLambda, degree):\n",
        "    '''\n",
        "    Compute learning curve\n",
        "\n",
        "    Arguments:\n",
        "        Xtrain -- Training X, n-by-1 matrix\n",
        "        Ytrain -- Training y, n-by-1 matrix\n",
        "        Xtest -- Testing X, m-by-1 matrix\n",
        "        Ytest -- Testing Y, m-by-1 matrix\n",
        "        regLambda -- regularization factor\n",
        "        degree -- polynomial degree\n",
        "\n",
        "    Returns:\n",
        "        errorTrains -- errorTrains[i] is the training accuracy using\n",
        "        model trained by Xtrain[0:(i+1)]\n",
        "        errorTests -- errorTrains[i] is the testing accuracy using\n",
        "        model trained by Xtrain[0:(i+1)]\n",
        "\n",
        "    Note:\n",
        "        errorTrains[0:1] and errorTests[0:1] won't actually matter, since we start displaying the learning curve at n = 2 (or higher)\n",
        "    '''\n",
        "\n",
        "    n = len(Xtrain);\n",
        "\n",
        "    errorTrain = np.zeros((n))\n",
        "    errorTest = np.zeros((n))\n",
        "    for i in range(2, n):\n",
        "        Xtrain_subset = Xtrain[:(i+1)]\n",
        "        Ytrain_subset = Ytrain[:(i+1)]\n",
        "        model = PolynomialRegression(degree, regLambda)\n",
        "        model.fit(Xtrain_subset,Ytrain_subset)\n",
        "\n",
        "        predictTrain = model.predict(Xtrain_subset)\n",
        "        err = predictTrain - Ytrain_subset;\n",
        "        errorTrain[i] = np.multiply(err, err).mean();\n",
        "\n",
        "        predictTest = model.predict(Xtest)\n",
        "        err = predictTest - Ytest;\n",
        "        errorTest[i] = np.multiply(err, err).mean();\n",
        "\n",
        "    return (errorTrain, errorTest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "juW1J7Av0NjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "\n",
        "    def __init__(self, alpha=0.01, regLambda=0.01, epsilon=0.0001, maxNumIters=10000):\n",
        "        '''\n",
        "        Constructor\n",
        "        '''\n",
        "        self.alpha = alpha\n",
        "        self.regLambda = regLambda\n",
        "        self.epsilon = epsilon\n",
        "        self.maxNumIters = maxNumIters\n",
        "        self.theta = None\n",
        "\n",
        "    def computeCost(self, theta, X, y, regLambda):\n",
        "        '''\n",
        "        Computes the objective function\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "            y is an n-dimensional numpy vector\n",
        "            regLambda is the scalar regularization constant\n",
        "        Returns:\n",
        "            a scalar value of the cost  ** make certain you're not returning a 1 x 1 matrix! **\n",
        "        '''\n",
        "        m = len(y)\n",
        "        h = self.sigmoid(X @ theta)\n",
        "        J = (-1/m) * (y.T @ np.log(h) + (1 - y).T @ np.log(1 - h)) + (regLambda / (2 * m)) * np.sum(theta[1:]**2)\n",
        "        return J.item()\n",
        "\n",
        "    def computeGradient(self, theta, X, y, regLambda):\n",
        "        '''\n",
        "        Computes the gradient of the objective function\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "            y is an n-dimensional numpy vector\n",
        "            regLambda is the scalar regularization constant\n",
        "        Returns:\n",
        "            the gradient, an d-dimensional vector\n",
        "        '''\n",
        "        m = len(y)\n",
        "        h = self.sigmoid(X @ theta)\n",
        "        grad = (1/m) * X.T @ (h - y) + (regLambda / m) * np.concatenate(([0], theta[1:]))\n",
        "        return grad\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "        Trains the model\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "            y is an n-dimensional numpy vector\n",
        "        '''\n",
        "        n, d = X.shape\n",
        "        X = np.concatenate((np.ones((n, 1)), X), axis=1)  # Add a column of ones to X for the bias term\n",
        "        self.theta = np.zeros((d + 1, 1))\n",
        "\n",
        "        for _ in range(self.maxNumIters):\n",
        "            gradient = self.computeGradient(self.theta, X, y, self.regLambda)\n",
        "            self.theta -= self.alpha * gradient\n",
        "\n",
        "            if np.linalg.norm(gradient) < self.epsilon:\n",
        "                break\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Used the model to predict values for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "        Returns:\n",
        "            an n-dimensional numpy vector of the predictions\n",
        "        '''\n",
        "        n = X.shape[0]\n",
        "        X = np.concatenate((np.ones((n, 1)), X), axis=1)  # Add a column of ones to X for the bias term\n",
        "        predictions = np.round(self.sigmoid(X @ self.theta))\n",
        "        return predictions.flatten().astype(int)\n",
        "\n",
        "    def sigmoid(self, Z):\n",
        "        '''\n",
        "        Computes the sigmoid function 1/(1+exp(-z))\n",
        "        '''\n",
        "        return 1 / (1 + np.exp(-Z))\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3BoD_6h0Gnx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}